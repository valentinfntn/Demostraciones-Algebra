\pdfminorversion=4
\documentclass[]{article}

% Packages/Macros %
\usepackage{amssymb,latexsym,amsmath}     % Standard packages
\usepackage{hyperref,comment,enumitem}

% Margins %
\addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{1.00in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\topmargin}{-.50in}

\newtheorem{theorem}{Teorema}
\newtheorem{proposition}{Proposicion}[theorem]
\newtheorem{definition}{Definicion}
\newenvironment{proof}{\noindent{\bf Prueba:}}{$\hfill \Box$ \vspace{10pt}}  

\newcommand{\iSection}[1]{
  \phantomsection
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\K}{
    \mathbb{K}
}
\newcommand{\ida}{\Longrightarrow}
\newcommand{\vuelta}{\Longleftarrow}
\newcommand{\base}{\text{$\mathcal{B}$}}
\newcommand{\basep}{\text{$\mathcal{B}^{\prime}$}}

\begin{document}

\newcommand{\W}[1]{\text{$W_{#1}$}}
\newcommand{\mtx}[2]{\text{$\mathbb{K}^{#1 \times #2}$}}
\newcommand{\espvec}{\text{ $\mathbb{K}-$espacio vectorial }}
\newcommand{\espgen}[1]{\text{$\langle #1 \rangle$}}
\newcommand{\incluye}[2]{\text{$#1 \subseteq #2$}}
\newcommand{\conjunto}[1]{\text{$\{ {#1} \}$}}
\newcommand{\li}{L.I.\:}
\newcommand{\tl}{T.L.\:}
\newcommand{\tq}{\text{ $|$ }}
\newcommand{\T}[1]{\text{$T({#1})$}}
\newcommand{\novacio}{\text{$\neq\emptyset$}\:}
\newcommand{\cero}{\text{$\vec{0}$}}
\newcommand{\deftl}{\text{Sean $V,W$ $\mathbb{K}-$espacio vectoriales, sea $T: V\to W$ transformacion lineal.}\:}
\newcommand{\N}{\text{$Nu(T)$}\:}
\newcommand{\I}{\text{$Im(T)$}\:}
\newcommand{\dm}[1]{\text{$dim(#1)$}\:}
\newcommand{\vc}[1]{\text{$v_{#1}$}}
\newcommand{\wc}[1]{\text{$w_{#1}$}}
\newcommand{\cb}{\text{$\begin{bmatrix} T \end{bmatrix}_{\basep}^{\base}$}}
\newcommand{\cbs}[3]{\text{$\begin{bmatrix} {#1} \end{bmatrix}_{#2}^{#3}$}}
\newcommand{\defaut}{\text{Sea $T:V\to V$} transformacion lineal y $V$ un $\mathbb{K}-$espacio vectorial}

\title{Demostraciones Algebra}
\maketitle

\begin{theorem}
    Sean $A,B \in \mtx{m}{n}$  matrices equivalentes por filas, entonces el sistema
    de ecuaciones $Ax=0$ y $Bx=0$ tienen exactamente las mismas soluciones. 
\end{theorem}
\begin{proof}
    Si $A \sim B \Longrightarrow \exists$ una sucesion de matrices tal que
    $A=A_0 \to A_1 \to \dots \to A_n=B$, donde cada $A_j$ se obtiene por
    medio de una operacion elemental por filas.\\
    Por lo tanto basta probar que $A_jx=0$ y $A_{j+1}x=0$.
    \begin{itemize}
        \item \underline{Caso $e_r^c$}: $a_{r1}x_1+a_{r2}x_2+ \dots +a_{rn}x_n = 0 \iff
        c \cdot a_{r1}x_1+c \cdot a_{r2}x_2+ \dots +c \cdot a_{rn}x_n = 0$, pero como $c \neq 0$
        $\Longrightarrow c \cdot (a_{r1}x_1+a_{r2}x_2+ \dots +a_{rn}x_n) = 0$, por lo tanto 
        ambos sistemas son iguales.
        \item \underline{Caso $e_{r,s}$}: es trivial pues ambas filas $r,s$ ya eran iguales a 0
        y lo siguen siendo.
        \item \underline{Caso $e_{r,s}^c$}: $(r + c \cdot s) = (a_{r1}+c\cdot a_{s1})x_1+
        (a_{r2}+c\cdot a_{s2})x_2+ \dots +(a_{rn}+c\cdot a_{sn})x_n = 0$ de la misma formas que en
        el primer caso como las filas $r,s$ son iguales a 0 por lo tanto la nueva fila r tambien lo es.
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $A \in \mtx{m}{n}$ con $m<n \Longrightarrow$ el sistema $Ax=0$
    tiene soluciones no triviales.
\end{theorem}
\begin{proof}
    Sea $R$ la MERF equivalente a $A \Longrightarrow$ los sistemas $Ax=0$ y $Rx=0$ tienen
    exactamente las mismas soluciones. Sea $r=$ la cantidad de filas no nulas de $R$
    $\Longrightarrow r \leq m$ y por lo tanto $r < n$ $\Longrightarrow$ hay $n-r>0$
    variables libres, por lo tanto hay soluciones no triviales.
\end{proof}

\begin{theorem}
    Sea $A \in \mathbb{K}^{n \times n}$. Entonces $A$ es equivalente por filas a las $Id \iff$
    $Ax=0$ tiene unicamente la solucion trivial.
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow):$ Si $A \sim Id$, estas tienen exactamente las mismas soluciones.
        Por lo tanto como $Idx=0$ admite unicamente la solucion trivial queda probado.
        \item $(\Longleftarrow):$ Sea $R$ la MERF $\sim A \Longrightarrow$ el sistema $Rx=0$
        tiene unicamente la solucion trivial. Sea $r=$ la cantidad de filas no nulas de $R \Longrightarrow$
        $n-r=0$ porque no tienen variables libres. Entonces cada fila $i$ tiene un 1 en la columna $k_i$
        por lo tanto $R=Id$.
    \end{itemize}
\end{proof}

\begin{theorem}
    Propiedades de la multiplicacion de matrices:
    \begin{enumerate}
        \item $A \in \mtx{m}{n}, B \in \mathbb{K}^{n\times p},C \in \mathbb{K}^{p\times q}$
        $\Longrightarrow (AB)C=A(BC)$.
        \item $A \in \mtx{m}{n} \Longrightarrow Id_mA = Id_nA = A$.
        \item $A,A^\prime \in \mtx{m}{n}, B,B^\prime \in \mathbb{K}^{n\times p} \Longrightarrow$
        $(A+A^\prime)B = AB+A^\prime B$ y $A(B+B^\prime) = AB+AB^\prime$.
        \item $A \in \mtx{m}{n}, B \in \mathbb{K}^{n\times p},\lambda \in \mathbb{K}$
        $\Longrightarrow \lambda \cdot (AB) = (\lambda A)B= A(\lambda B)$ 
    \end{enumerate}
\end{theorem}
\newpage
\begin{theorem}
    Sea $e$ una operacion elemental por filas y sea $E=e(Id)$ la matriz elemental asociada.
    Entonces para toda $A \in \mtx{n}{n}$ se cumple que $e(A)=E \cdot A$. 
\end{theorem}
\begin{proof}
    Tenemos que el elemento $i,j$ de $e(A)$ es el mismo que el de la matriz $EA$ para cada
    operacion elemental, osea $(e(A))_{ij}=(EA)_{ij}$.
    \begin{itemize}
        \item \underline{Caso $e_r^c$}:
        \begin{align*}
            &\text{Sabemos que } (e(A))_{ij}=
            \begin{cases}
                A_{ij} \text{ si } i \neq r\\
                cA_{ij} \text{ si } i = r
            \end{cases}\\
            &\text{Veamos }(EA)_{ij}=\sum_{k=1}^{m}E_{ik}A_{kj}
            \text{ (si $i \neq k \Longrightarrow E_{ik}=0$)}\\
            &= E_{ii}A_{ij}=
            \begin{cases}
                A_{ij} \text{ si } i \neq r\\
                cA_{ij} \text{ si } i = r
            \end{cases}\\
        \end{align*}
        \item \underline{Caso $e_{r,s}$}:
        \begin{align*}
            &\text{Sabemos que }(e(A))_{ij}=
            \begin{cases}
                A_{ij} \text{ si } i \neq r,s\\
                A_{sj} \text{ si } i = r\\
                A_{rj} \text{ si } i = s
            \end{cases}\\
            &(EA)_{ij}=\sum_{k=1}^{m} E_{ik}A_{kj}
            \text{ , donde } E_{ik}=
            \begin{cases}
                1 \text{ si } i=k \vee i=r,s \vee k=r,s\\
                0 \text{ caso contrario}
            \end{cases}\\
            &\text{Veamos $(EA)_{ij}$ en cada caso:}
            \begin{cases}
                \text{si } i \neq r,s \Longrightarrow (EA)_{ij} = A_{ij}\\
                \text{si } i=r \Longrightarrow (EA)_{ij} = E_{is}A_{sj} = A_{sj}\\
                \text{si } i=s \Longrightarrow (EA)_{ij} = E_{ir}A_{rj} = A_{rj}
            \end{cases}
        \end{align*}
        \item \underline{Caso $e_{r,s}^c$}:
        \begin{align*}
            &\text{Sabemos que } e(A)_{ij}=
            \begin{cases}[within]
                A_{ij} \text{ si } i \neq r\\
                A_{rj} + cA_{sj} \text{ si } i=r
            \end{cases}\\
            &(EA)_{ij}=\sum_{k=1}^{m} E_{ik}A_{kj}
            \text{ , donde } E_{ik}=
            \begin{cases}
                1 \text{ si } i=k\\
                c \text{ si } i=r \wedge j=s\\
                0 \text{ caso contrario}
            \end{cases}\\
        \end{align*}
    \end{itemize}
\end{proof}
def
\begin{theorem}
    Sean $A,B \in \mtx{n}{n}$:
    \begin{enumerate}
        \item Si $A$ es inversible $\Longrightarrow A^{-1}$ tambien lo es y $(A^{-1})^{-1}=A$.
        \item Si $A,B$ son inversibles $\Longrightarrow AB$ es inversible y $(AB)^{-1}=B^{-1}A^{-1}$. 
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item $A \cdot A^{-1} = A^{-1} \cdot A = Id \Longrightarrow A^{-1}$ inversible y $(A^{-1})^{-1}$ es $A$.
        \item $(AB)\cdot (B^{-1}A^{-1})=A(BB^{-1})A^{-1} = A(Id)A^{-1}=AA^{-1}=Id$.
    \end{enumerate}
\end{proof}

\newpage

\begin{theorem}
    Toda matriz elemental $E$ es inversible.
\end{theorem}
\begin{proof}
    Sea $e$ la operacion elemental por fila correspondiente a $E$ y sea $e^{\prime}$ la operacion elemental inversa
    (sabemos que existe por teorema). Por lo tanto sea $E^{\prime}=e^{\prime}(Id)$
    \begin{align*}
        &Id=e^{\prime}(e(Id))=e^{\prime}(E)=E^{\prime}E\\
        &Id=e(e^{\prime}(Id))=e(E^{\prime})=EE^{\prime}\\
        &\Longrightarrow E \text{ es inversible y su inversa es } E^{\prime}
    \end{align*}
\end{proof}
\begin{theorem}
    Sea $A \in\mtx{n}{n}$ entonces son equivalentes:
    \begin{enumerate}
        \item $A$ es inversible.
        \item $A$ es equivalente por filas a la $Id$.
        \item $A$ es producto de matrices elementales.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $[1 \Longrightarrow 2]$ Sea $R$ la MERF $\sim A \Longrightarrow$ existen matrices elementales
        $E_1, \dots ,E_k$ talque $R=E_k\cdots E_2E_1A$. Como las matrices elementales $E_j$ y $A$ son inversibles
        $\Longrightarrow R$ tambien lo es $\Longrightarrow R$ no tiene filas nulas por lo tanto $R=Id$.
        \item $[2 \Longrightarrow 3]$ Si $A \sim Id \Longrightarrow Id \sim A \Longrightarrow$ existen P productos
        de matrices elementales talque $A = P \cdot Id = E_1E_2 \cdots E_k \cdot Id$.
        \item $[3 \Longrightarrow 1]$ Supongamos $A=E_1 \cdots E_k$ donde $E_j$ es una matriz elemental. Como cada
        $E_j$ es inversible y el producto de matrices inversibles tambien lo es $\Longrightarrow$ A es inversible.
    \end{itemize}
\end{proof}

\begin{theorem}
    Sean $A,B \in\mtx{m}{n}$. Entonces $B$ es equivalente por filas a $A \iff \exists$ $P$ matriz inversible
    $m \times m$ talque $B=P \cdot A$  
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow):$ Si $B \sim A$ sabemos que $B=E_kE_{k-1}\cdots E_1$ y como cada $E_i$ es inversible
        el producto de matrices inversibles tambien lo es.
        \item $(\Longleftarrow):$ Sea $P$ inversible talque $B=PA$ como $P$ es producto de matrices elementales
        $\Longrightarrow B=E_k \cdots E_1 A \Longrightarrow$ $B$ se obtiene de $A$ haciendo operaciones elementales
        $\Longrightarrow B \sim A$.  
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $A\in\mtx{n}{n}$. Entonces son equivalentes: 
    \begin{enumerate}
        \item $A$ es inversible.
        \item El sistema $Ax=0$ tiene una unica solucion (la trivial).
        \item $\forall \: b \in\mathbb{K}^{n\times 1}$ el sistema no-homogeneo $Ax=b$ tiene una unica solucion.
    \end{enumerate}
\end{theorem}
\newpage
\begin{proof}
    \begin{itemize}
        \item $[1 \iff 2]$ Sabemos que $A$ es inversible $\iff A \sim Id \iff$ el sistema $Ax=0$ tiene
        como unica solucion la trivial.
        \item $[1 \Longrightarrow 3]$ Sea $b \in\mathbb{K}^{n\times 1}$ , como $A$ es inversible $\Longrightarrow$
        $\exists A^{-1}$. Por lo tanto sea $x_0= A^{-1}b \in\mathbb{K}^{n\times 1}$ $\Longrightarrow Ax_0=A(A^{-1}b)=b$.\\
        Veamos que es unica, para eso supongamos que existe otra solucion $x_1 \Longrightarrow Ax_1=b$
        $\Longrightarrow Ax_1=b=Ax_0$ ahora multiplicamos por la inversa $\Longrightarrow A^{-1}Ax_0=A^{-1}Ax_1$
        $\Longrightarrow x_0=x_1$
        \item $[3 \Longrightarrow 2]$ Como tiene solucion para todo $b$ tomo $b=0$ por lo tanto, obviamente, tiene una unica
        solucion por hipotesis. 
    \end{itemize}
\end{proof}

\newpage

\begin{theorem}
    Si $W \subseteq V$ y $W\novacio$. Entonces $W$ es un $\mathbb{K}-$subespacio vectorial de $V$ $\iff \forall v,w \in W$
    y $\forall c \in \mathbb{K}$ el vector $[c \cdot v + w] \in W$ 
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow):$ Si $W$ es un subespacio vectorial y $c \in \mathbb{K}$ , $v,w \in W \Longrightarrow$
        $c \cdot v \in W \Longrightarrow c \cdot v + w \in W$
        \item $(\Longleftarrow):$ Supongamos que $\forall v,w \in W$ y  $\forall c \in \mathbb{K} : c \cdot v + w \in W$ veamos contiene
        al $\cero$, que es cerrado para la suma y el producto por escalar.
        \begin{enumerate}
            
            \item Como $W \novacio \Longrightarrow \exists w \in W \Longrightarrow (-1) \cdot w + (1) \cdot w \in W$
            $\Longrightarrow \cero \in W$.
            \item Tomamos $c=1$ por lo tanto $(1) \cdot v + w = v + w \in W$ por lo tanto la suma esta bien definida.
            \item Como $\cero \in W$ tomamos $w=\cero$ por lo tanto $c \cdot v + \cero = c \cdot v \in W$
            entonces el producto esta bien definido.
        \end{enumerate}
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec. Entonces la interseccion de subespacios de $V$ es un subespacio vectorial de $V$.
\end{theorem}
\begin{proof}
    \begin{align*}
        \text{Sea $\{ W_i \}_{i \in I}$ , donde $W_i$ es un subespacio vectorial de V. Entonces sea } W = \bigcap_{i \in I}W_i.
    \end{align*}
    Para ver que $W$ es un subespacio veamos que si $v,w \in W$ , $c \in \K \ida c \cdot v + w \in W$.\\
    Si $v,w \in W \ida v,w \in W_i$ para todo $i \in I$ y como todo $W_i$ es un subespacio $\ida c \cdot v + w \in W_i$
    $\ida c \cdot v + w \in W$.
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec y sean $v_1, \dots , v_k \in V$.\\
    Entonces $W=\{ c_1v_1+ \dots + c_kv_k | c_i \in \K\}$ = Conjunto de todas la combinaciones lineales,
    es un subespacio vectorial de $V$.
\end{theorem}
\begin{proof}
    Tomemos $v,w \in W$ , $c \in \K$ veamos $c \cdot v + w \in W$.
    \begin{align*}
        &\text{Sean } v= c_1v_1+ \dots c_kv_k \text{ y } w = d_1v_1 + \dots d_kv_k \ida \\
        &c \cdot v + w = (c \cdot c_1v_1+ \dots c \cdot c_kv_k) + (d_1v_1 + \dots d_kv_k)\\
        &= (c \cdot c_1 + d_1)v_1 + \dots (c \cdot c_k + d_k)v_k \\
        &\ida c \cdot v + w \text{ es una combinacion lineal, por lo tanto} \in W
    \end{align*}
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec de dimension finita y $W$ subespacio de $V$. Todo subconjunto \li de $W$ es
    finito y se puede completar a una base. 
\end{theorem}
\begin{proof}
    Si $S_0 \subseteq W$ es \li $\ida S_0$ es un conjunto L.I de $V$ y si $dim(V)=n$, sabemos que $|S_0|\leq n$,
    osea es finito. Queremos extenderlo a una base de la siguiente forma:
    \begin{itemize}
        \item Si $S_0$ genera $W$ ya es una base.
        \item Si $S_0$ no genera $W \ida \exists w_1 \in W$ talque $w_1 \notin \espgen{S_0}$, vimos que si
        agregamos algo que no esta en el espacio generado este conjunto sigue siendo \li\\
        Repetimos este paso hasta algun $S_k = S_0 \cup w_1 \cup \cdots \cup w_k$ talque $\espgen{S_k} = W$.
        Como este conjunto es por definicion \li y genera este es una base.  
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec, sea $S=\{ v_1, \dots ,v_m \}$ un conjunto de generadores de V. Entonces existe
    $B \subseteq S$ que es una base de $V$. 
\end{theorem}
\begin{proof}
    Vamos a definir inductivamente subconjuntos $G_j \subseteq S$ ($j=1,\dots,m$) que sean \li
    \begin{itemize}
        \item $(j=1):$
        \begin{align*}
            G_1=
            \begin{cases}
                \{v_1\} \quad si \quad v_1 \neq 0\\
                \emptyset \quad si \quad v_1 = 0
            \end{cases}
            \ida G_1 \text{ es \li}
        \end{align*}
        \item $(j=2):$
        \begin{align*}
            G_2=
            \begin{cases}
                G_1 \cup {v_2} \quad si \quad v_2 \notin \espgen{G_1}\\
                G_1 \quad\quad si \quad v_2 \in \espgen{G_1}
            \end{cases}
            \ida G_2 \text{ es \li en cualquier caso}
        \end{align*}
        \item $(j+1):$
        \begin{align*}
            G_{j+1}=
            \begin{cases}
                G_j \cup {v_{j+1}} \quad si \quad v_{j+1} \notin \espgen{G_j}\\
                G_j \quad\quad si \quad v_{j+1} \in \espgen{G_j}
            \end{cases}
            \ida \text{ en ambos casos $G_{j+1}$ es \li}
        \end{align*}
    \end{itemize}
    $\ida G_m$ es \li y $\espgen{G_m} = \{ v_1,\dots,v_m \} = V \ida \base = G_m \subseteq S$ y la dimension
    de $V = |G_m|$.
\end{proof}

\begin{theorem}
    Sean $\W{1} , \W{2}$ dos subespacios de $V$:
    \begin{enumerate}
        \item $\W{1} + \W{2}$ es un subespacio de $V$.
        \item $\W{1} + \W{2}$ es el menor subespacio (respecto a la inclusion) que contiene a $\W{1}+\W{2}$.
        \item Si $\{ v_i \}_{i \in I}$ es un conjunto de generadores de \W{1} y $\{ w_j \}_{j \in J}$ es un conjunto
        de generadores de $\W{2} \ida \{ v_i \}_{i \in I} \cup \{ w_j \}_{j \in J}$ es un conjunto de generadores de $\W{1}+\W{2}$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item $\W{1} + \W{2} \novacio$ pues $\cero \in \W{1}$ y $\cero \in \W{2}\ida \cero=\cero+\cero \in \W{1}+\W{2}$.
        Sean $v_1,v_2 \in \W{1}+\W{2}$ queremos ver que $c \cdot v_1 + v_2 \in \W{1} + \W{2}$.
        \begin{align*}
            &v_1 = x_1 + y_1 \text{ , } v_2= x_2 + y_2 \text{ con } x_1,x_2 \in \W{1} \subseteq V
            \text{ y } y_1,y_2 \in \W{2} \subseteq V\\
            &\ida c \cdot v_1 + v_2 = c \cdot (x_1 + y_1) + (x_2 + y_2) = 
            (cx_1 + x_2) \in  \W{1} + (cy_1 + y_2) \in \W{2}\\
            &\ida c \cdot v_1 + v_2 \in \W{1} + \W{2}
        \end{align*}
        \item $\W{1} \subseteq \W{1} + \W{2}$ pues si $w_1 \in \W{1} \ida w_1 + \cero \in \W{1}+\W{2}$ y del mismo modo con \W{2}.
        Por lo tanto $\W{1} \cup \W{2} \subseteq \W{1} + \W{2}$, basta ver que es el menor subespacio que lo contiene.\\
        Sea $U$ un subespacio de $V$ talque $\W{1} \cup \W{2} \subseteq U$, queremos ver que $\W{1} + \W{2} \subseteq U$.\\
        Sea $v \in \W{1} + \W{2} \ida v=x+y$ con $x \in \W{1} \subseteq U$, $y \in \W{2} \subseteq U$ osea $v \in U$
        $\ida \W{1} + \W{2} \subseteq U$.
        \item $\{v_i\}_{i \in I}$ genera \W{1}, $\{w_j\}_{j \in J}$ genera $\W{2} \ida S=$
        $\{v_i\}_{i \in I} \cup \{w_j\}_{j \in J}$ genera $\W{1} + \W{2}$.
        \begin{align*}
            &\text{Sea } v \in \W{1} + \W{2} \ida v=x+y \text{ con } x \in \W{1} , y \in \W{2}\\
            &\ida x=\sum_{i \in I} \alpha_i v_i \: , \: y=\sum_{j \in J} \beta_j w_j
            \quad (\text{ con } \alpha_i,\beta_j \in \K)\\
            &\ida v=x+y=\sum_{i \in I}\alpha_i v_i + \sum_{j \in J}\beta_j w_j\\
            &\text{Por lo tanto $v$ es combinacion lineal de elementos de $S$}
        \end{align*}
    \end{enumerate}
\end{proof}
\begin{theorem}
    Sea $V$ un \espvec , sean $\W{1},\W{2}$ subespacios de $V$ y de dimension finita.\\
    Entonces $\W{1}+\W{2}$ es un subespacio y $dim(\W{1}+\W{2}) = dim(\W{1}) + dim(\W{2}) - dim(\W{1} \cap \W{2})$
\end{theorem}
\begin{proof}
    $\W{1}\cap\W{2}$ es un subespacio de \W{1} (y de \W{2}) por lo tanto es de dimension finita.
    Sea \conjunto{u_1,\dots,u_k} base de $\W{1} \cap \W{2} \ida$ \conjunto{u_1,\dots,u_k} es \li en \W{1}
    $\ida$ se puede extender a una base de \W{1}, analogamente con \W{2}.\\
    Osea existen \conjunto{v_1,\dots,v_n} $\subseteq$ \W{1} tales que \conjunto{u_1,\dots,u_k,v_1,\dots,v_n}
    es una base de \W{1}. De la misma forma existen \conjunto{w_1,\dots,w_m} $\subseteq$ \W{2} tales que
    \conjunto{u_1,\dots,u_k,w_1,\dots,w_m} es base.\\
    $\ida \W{1}+\W{2}$ es generado por $\base= \conjunto{u_1,\dots,u_k,v_1,\dots,v_n,w_1,\dots,w_m}$
    es $\base_1 \cup \base_2$. Veamos que este conjunto es \li
    \begin{align*}
        &\text{Si } \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{n} b_i v_i + \sum_{i=1}^{m} c_i w_i = 0\\
        &\ida \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{n} b_i v_i = \underline{- \sum_{i=1}^{m} c_i w_i} \in \W{1}\cap\W{2}\\
        &\text{Como \conjunto{u_1,\dots,u_k} es base de } \W{1}\cap\W{2} \ida - \sum_{i=1}^{m} c_i w_i = \sum_{i=1}^{k} d_iu_i\\
        &\text{Reemplazando } \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{n} b_i v_i + \sum_{i=1}^{k} (-d_i)u_i = 0\\
        &\sum_{i=1}^{k} (a_i-d_i)u_i + \sum_{i=1}^{n} b_i v_i = 0
    \end{align*}
    Como \conjunto{u_1,\dots,u_k,v_1,\dots,v_n} es base de \W{1} el conjunto es $\li\ida a_i=d_i$ y $b_i=0 ,\:\forall\: i$.\\
    Volviendo a la sumatoria con este resultado sabemos que:
    \begin{align*}
        \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{m} c_i w_i = 0
    \end{align*}
    Como \conjunto{u_1,\dots,u_k,w_1,\dots,w_m} es base de \W{2} el conjunto es \li $\ida$ todos los escalares son 0.
    Osea $\beta = \conjunto{u_1,\dots,u_k,v_1,\dots,v_n,w_1,\dots,w_m}$ es \li y por lo tanto es base de \W{1}+\W{2}
    \begin{align*}
        &\ida dim(\W{1}+\W{2})<\infty \text{ y } dim(\W{1}+\W{2})=k+n+m\\
        &dim(\W{1})=k+n \text{ , } dim(\W{2})=k+m \text{ , } dim(\W{1}\cap\W{2})=k\\
        &\ida dim(\W{1}+\W{2}) = dim(\W{1})+dim(\W{2})-dim(\W{1}\cap\W{2})
    \end{align*}
\end{proof}

\begin{proposition}
    Si $V=\W{1} \oplus \W{2} \ida$ para cada $v\in V$ : $\exists$ unicos elementos $x \in \W{1} , y \in\W{2}$
    tales que $v=x+y$.
\end{proposition}
\begin{proof}
    Como $V=\W{1}+\W{2}$ sabemos que existen $w_1 \in \W{1}, w_2 \in \W{2}$ tales que $v=w_1=w_2$.
    Si ademas $v=u_1+u_2 \ida \cero= v-v= (w_1+w_2)-(u_1+u_2)=(w_1-u_1)+(w_2-u_2)$
    $\ida w_1-u_1 = w_2-u_2 \in \W{1}\cap\W{2}= \conjunto{0}$ (pues es suma directa) $\ida w_1=u_1 , w_2=u_2$.
\end{proof}

\begin{theorem}
    Sean \W{1},\W{2} subespacios vectoriales de $V$ y sean $\base_1,\base_2$ bases de \W{1} y \W{2}.\\
    Entonces son equivalentes:
    \begin{enumerate}
        \item $V=\W{1}\oplus\W{2}$.
        \item $\base = \base_1 \cup \base_2$ es una base de $V$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $[1 \ida 2]:$ Asumimos que $V=\W{1}\oplus\W{2} \ida \base = \base_1 \cup \base_2$ es un conjunto de generadores de
        $\W{1}+\W{2}$ bastar ver que es \li\\
        Sabemos que $dim(V)=dim(\W{1}\oplus\W{2})=dim(\W{1})+dim(\W{2})-dim(\conjunto{0})$
        \begin{align*}
            &\sum_{i \in I} a_i v_i + \sum_{j \in J} b_j w_j = 0 \ida
            \underset{\in \W{1}}{\sum_{i \in I} a_i v_i} = \underset{\in \W{2}}{-\sum_{j \in J} b_j w_j} \ida \W{1}\cap\W{2}=\conjunto{0}\\
            &\sum_{i \in I} a_i v_i =  \cero \text{ , } \sum_{j \in J} b_j w_j = \cero\\
            &\ida a_i=0,b_j=0 \text{ $\forall i \in I, \forall j \in J$ pues ambos $v_i,w_j$ son base y por lo tanto son } \li
        \end{align*}
        \item $[2 \ida 1]:$ Asumimos que $\base=\base_1 \cup \base_2$  base de $V \ida V=\W{1}+\W{2}$ pues sabemos que:
        \begin{align*}
            v=\sum_{i \in I}a_i v_i + \sum_{j \in J}b_j w_j
        \end{align*}
        Necesitamos ver que $\W{1}\cap\W{2}=\conjunto{0}$ para ver que la suma es directa. Sea $v\in\W{1}\cap\W{2}:$
        \begin{align*}
            &\ida v=\sum_{i \in I}a_i v_i = \sum_{j \in J}b_j w_j\\
            &\ida 0=\sum_{i \in I}a_i v_i - \sum_{j \in J}b_j w_j\\
            &(\text{como ambos conjuntos son L.I. esto solo pasa si } a_i=0 \:\forall i \in I,b_j=0 \:\forall j \in J)\\
            &\ida v=0\ida \W{1}\cap\W{2}=\conjunto{0}
        \end{align*}
    \end{itemize}
\end{proof}

\newpage
\begin{definition}
    Dados V,W espacio vectoriales sobre $\K$.\\
    Una transformacion lineal de V en W es una funcion, $T:V \to W$ que satisface:
    \begin{itemize}
        \item $T(v+w)=T(v)+T(w)$ \quad $\forall v,w \in V$
        \item $T(c\cdot v)= c\cdot T(v)$ \quad $\forall v\in V, \forall c \in \K$
    \end{itemize}
    De esas propiedades se deducen las siguientes:
    \begin{itemize}
        \item $T(\cero_V)=\cero_W$
        \item $T(-v)=-T(v)$ \quad $\forall v \in V$
    \end{itemize}
\end{definition}

\begin{theorem}
    Sean V,W \espvec , $T:V\to W$ una \tl
    \begin{enumerate}
        \item Si $\incluye{U}{V}$ es un subespacio de $V \ida T(U)$ es un subespacio de $W$.
        \item Si $\incluye{Z}{W}$ es un subespacio de $W \ida T^{-1}(Z)=\conjunto{v \in V \tq T(w)\in Z}$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Sea $\T{U} = \conjunto{w \in W \tq \exists v \in U \text{ con } \T{v}=w} \novacio$ , veamos
        que \T{U} es subespacio:
        \begin{align*}
            &\text{Como } \cero \in U \text{ (por ser subespacio) y } \T{\cero}=0 \ida \cero \in \T{U}\\
            &\text{Sean } w_1,w_2 \in \T{U}: \ida \exists v_1,v_2 \in U \text{ tales que } w_1=\T{v_1},w_2=\T{v_2}\\
            &\ida c\cdot w_1 + w_2 = c\T{v_1}+\T{V_2} = \T{c\cdot v_1 + v_2}\\
            &\text{Como } c\cdot v_1+v_2 \in U \ida c\cdot w_1 + w_2 \in \T{U}
        \end{align*}
        \item $\cero \in T^{-1}(Z)$ pues $\T{0} = \cero \in Z$ por lo tanto $T^{-1}(Z) \novacio$.
        Sean $v_1,v_2 \in T^{-1}(Z) , c \in \K$ veamos que $c\cdot v_1 + v_2 \in T^{-1}(Z)$:
        \begin{align*}
            &\T{c\cdot v_1+v_2}=c\T{v_1}+\T{v_2} \in defZ\\
            &\ida c\cdot v_1+v_2 \in T^{-1}(Z)
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}
    \deftl\\
    Entonces:
    \begin{enumerate}
        \item \N es un subespacio de V.
        \item \I es un subespacio de W.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item
        \begin{align*}
            &\N\novacio \text{ , pues } \cero\in\N \text{ porque } \T{\cero}=\cero\\
            &\text{Sean } v_1,v_2 \in \N , c \in\K\\
            &\ida \T{cv_1+v_2}=c\T{v_1}+\T{v_2}=\cero \ida c\cdot v_1 + v_2 \in \N
        \end{align*}
        \item
        \begin{align*}
            &\incluye{\I}{W} , \I\novacio \text{ pues } \cero\in\I \text{ , } \cero\in\T{\cero}\\
            &\text{Sean } w_1,w_2 \in \I , c\in\K \\
            &\ida \exists v_1,v_2 \in V \text{ tales que } \T{cv_1+v_2}=c\T{v_1}+\T{v_2}=c\cdot w_1 + w_2 \in \I 
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}
    \deftl\\
    (con $dim(V)<\infty$) Entonces:
    \begin{align*}
        dim(V)=dim(\I)+dim(\N)
    \end{align*}
\end{theorem}
\begin{proof}
    Sea \conjunto{v_1,\dots,v_k} base de \incluye{\N}{V}. Como el conjunto es \li podemos completarlo
    a una base de $V$. Sean \incluye{\conjunto{v_{k+1},\dots,v_n}}{V} tal que \conjunto{v_1,\dots,v_k,v_{k+1},\dots,v_n}
    es una base de $V$.\\
    Probemos \conjunto{\T{\vc{k+1}},\dots,\T{\vc{n}}} es base de \I :
    \begin{align*}
        &\text{Sea } w \in \I \ida \exists v \in V \text{ tal que } w=\T{v} \ida v=\sum_{j=1}^{n}c_j\vc{j}\\
        &\T{v}=\sum_{j=1}^{n}c_j\T{\vc{j}}=\sum_{j=k+1}^{n}c_j\T{\vc{j}} \quad \text{ (porque $\T{\vc{j}}=0$ si $j\leq k$)}\\
        &\ida w \text{ es combinacion lineal de } \conjunto{\T{\vc{k+1}},\dots,\T{\vc{n}}}  
    \end{align*}
    Falta ver que el conjunto es $\li :$
    \begin{align*}
        &\ida b_{k+1}\T{\vc{k+1}}+\cdots+b_n\T{\vc{n}}=\underset{\vc{0}}{\T{\underbrace{b_{k+1}v_{k+1}+\cdots+b_nv_n}}}=0\\
        &\ida \vc{0}\in \N \ida \vc{0}=\sum_{j=k+1}^{n}b_jv_j=\sum_{j=1}^{k}c_iv_i
        \text{ pues \conjunto{\vc{1},\dots,\vc{k}} es base de \N}\\
        &\ida \sum_{j=k+1}^{n}b_jv_j+\sum_{j=1}^{k}(-c_i)v_i = 0
    \end{align*}
    Como la segunda parte es $\li \ida b_j=0$ , $\forall j=\conjunto{k+1,\cdots,n}$. Con esto probamos que
    \conjunto{\T{\vc{k+1}},\dots,\T{\vc{n}}} es \li y como genera es base de \I.\\
    Por lo tanto $\dm{\I}=n-k$ , ademas sabiamos que $\dm{\N}=k$
    \begin{align*}
        \ida \dm{\N}+\dm{\I}=k+(n-k)=n=\dm{V}
    \end{align*}
\end{proof}

\begin{definition}
\mbox{}
    \begin{itemize}
        \item El Rango Fila (A) = \dm{\text{espacio fila de A}}
        \item El Rango Columna (A) = \dm{\text{espacio columna de A}}
    \end{itemize}
\end{definition}

\begin{theorem}
    Sea $A \in \mtx{m}{n}$ Entonces: Rango fila de A = Rango columna de A
\end{theorem}
\begin{proof}
    Sea $T:\K^{n}\to\K^{m}$ la \tl definida por $\T{x}=A \cdot x$ , $\N=\conjunto{x \in \K^n : Ax=0}$.\\
    Sabemos que $A$ se puede reducir a una MERF $R$.
    \begin{align*}
        &\ida \text{espacio fila de A = espacio fila de R}\\
        &\ida \dm{\text{espacio fila de A}} = k \text{ (cantidad de filas no nulas)}\\
        &\ida \dm{\N}=n-k \text{ , Rango Fila (A) = } k\\ 
        &\ida \dm{\K^n}=n = \dm{\N}+ \text{ Rango Fila (A)}
    \end{align*}
    Por otra parte $T:\K^{n}\to\K^{m}$ , $x \to Ax$ entonces \I es generada por las columnas de A:
    \begin{align*}
        &\K^m \ida Ax =
        \begin{pmatrix} a_{11} \cdots a_{1n} \\ \vdots\quad\ddots\quad \vdots \\ a_{m1} \cdots a_{mn} \end{pmatrix}
        \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} =
        x_1 \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}+
        x_2 \begin{pmatrix} a_{12} \\ \vdots \\ a_{m2} \end{pmatrix}+
        \cdots +
        x_n \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}\\
        & \T{e_j}= \begin{pmatrix} a_{1j} \\ \vdots \\ a_{mj} \end{pmatrix}
        \text{ siendo $e_j$ el vector canonico que devuelve la columna $j$}\\
        \\
        &\ida \conjunto{e_1,e_2,\dots,e_{n-1},e_n} \text{ es base de } \K^n
        \ida \conjunto{\T{e_1},\dots,\T{e_n}} \text{ genera }\I\\
        \\
        &\ida \dm{\I}=\dm{\text{espacio columna de A}}=\text{Rango Columna (A)}
    \end{align*}
    Ahora el teorema dice que $\dm{\K^n}=\dm{\N}+\text{Rango Columna (A)}$. Pero antes vimos que
    $\dm{\K^n}=\dm{\N}+\text{Rango Columna (A)} \ida$ \underline{Rango Fila (A)= Rango Columna (A)}
\end{proof}
\begin{proposition}
    \deftl\\
    Entonces:
    \begin{itemize}
        \item T es inyectiva $\iff \N = \conjunto{0} \iff$ nulidad(T) = 0
        \item T es sobreyectiva $\iff \I = W \iff$ rango(T) = \dm{W}
    \end{itemize}
\end{proposition}

\begin{theorem}
    \deftl\\
    Entonces:
    \begin{enumerate}
        \item Si T es inyectiva y \conjunto{v_1,\dots,v_n} es \li en V
        $\ida$ \conjunto{\T{v_1},\dots,\T{v_n}} es \li en W.
        \item Si T es sobreyectiva y \conjunto{v_1,\dots,v_n} genera V $\ida$
        \conjunto{\T{v_1},\dots,\T{v_n}} genera W.
        \item En general si \conjunto{v_1,\dots,v_n} genera V $\ida$
        \conjunto{\T{v_1},\dots,\T{v_n}} genera \I.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Sean $c_1,\dots,c_r \in \K$ tales que $c_1\T{v_1}+\cdots+c_r\T{v_r}=0$ (como T es lineal)\\
        $\ida \T{c_1v_1+\cdots+c_rv_r}=0 \ida [c_1v_1+\cdots+c_rv_r] \in \N=\conjunto{0}$ (por ser inyectiva)\\
        $\ida c_1v_1+\cdots+c_rv_r=0 \ida c_1=0,\cdots,c_r=0$.
        \item Caso particular de $(3)$ donde $\I=W$.
        \item Sea $w \in \I \ida \exists v \in V$ talque $w=\T{v}$ , como \conjunto{v_1,\cdots,v_s} genera $V$
        \begin{align*}
            &\ida v= c_1v_1+\cdots+c_sv_s \quad \text{(con $c_i \in \K$)}\\
            &\ida w=\T{v}=\T{c_1v_1+\cdots+c_sv_s}=c_1\T{v_1}+\cdots+c_s\T{v_s}\\
            &\ida w \in \espgen{\conjunto{\T{v_1},\cdots,\T{v_s}}} \quad \text{(esta en el espacio generado)}\\
            &\ida \I \subseteq \espgen{\conjunto{\T{v_1},\cdots,\T{v_s}}} \subseteq \I\\
            &\ida \underline{\I=\espgen{\conjunto{\T{v_1},\cdots,\T{v_s}}}}  
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}
    \deftl\\
    (talque $\dm{V}=\dm{W}<\infty$). Entonces son equivalentes:
    \begin{enumerate}
        \item T es un isomorfismo.
        \item T es inyectiva.
        \item T es sobreyectiva.
        \item Si \conjunto{v_1,\cdots,v_n} es una base de V $\ida$ \conjunto{\T{v_1},\cdots,\T{v_n}} es una base de W.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $[1 \ida 2]:$ obvio pues es isomorfismo cuando tanto inyectiva como sobreyectiva.
        \item $[2 \ida 3]:$ Si T es inyectiva $\ida \N=\conjunto{0}\ida nulidad(T)=0$.\\
        Por teorema de las dimensiones sabemos que:
        \begin{align*}
            &n=\dm{V}=\dm{\N}+\dm{\I}\\
            &\ida n = 0 + \dm{\I} = \dm{W}\\
            &\ida \text{ T sobreyectiva (porque $\forall\: \incluye{S}{V}$ : $\dm{S}=\dm{W}$ es el mismo W)}
        \end{align*}
        \item $[3 \ida 1]:$ Supongamos T sobreyectiva $\ida\I=\dm{W}$, nuevamente por el teorema de las dimesiones
        sabemos que $\dm{\N}=0 \ida \N=\conjunto{0}$. Por lo tanto T es inyectiva $\ida T$ es biyectiva.
        \item $[1 \ida 4]:$ Sea \conjunto{v_1,\cdots,v_n} una base de $V \ida$ \conjunto{v_1,\cdots,v_n} es \li
        y como es inyectiva (por ser biyectiva) $\ida \conjunto{\T{v_1},\cdots,\T{v_n}}$ tambien es \li\\
        Ademas \conjunto{v_1,\cdots,v_n} genera $V$ y como es sobreyectiva $\ida \conjunto{\T{v_1},\cdots,\T{v_n}}$
        genera $W$.
        \item $[4 \ida 2]:$ Sea $\vc{1} \in \N$ , supongamos que $\vc{1}\neq0 \ida \conjunto{\vc{1}}$ es \li por lo tanto
        se puede extender a una base $\ida \exists \conjunto{\vc{1},\cdots,\vc{n}}$ base de $V
        \ida \conjunto{\T{\vc{1}},\cdots,\T{\vc{n}}}$ es base de $W \ida$ en particular $\T{\vc{1}}\neq0$ \:
        (ABSURDO pues $\vc{1}\in\N$, osea $\T{\vc{1}}=0$).
    \end{itemize}
\end{proof}
\begin{proposition}
    Sean V,W $\K-$espacio vectoriales de dimension finita. Entonces:
    \begin{align*}
        \exists\: T:V\to W \text{  isomorfismo} \iff \dm{V}=\dm{W}
    \end{align*}
\end{proposition}

\begin{definition}
    Sean V,W espacios vectoriales sobre $\K$. Sean $\base=\conjunto{\vc{1},\cdots,\vc{n}}$ base ordenada de V y
    $\basep=\conjunto{\wc{1},\cdots,\wc{m}}$ base ordenada de W. Sea $T:V\to W$ \tl
    \begin{align*}
        \text{Supongamos que } T(\vc{j})=\sum_{i=1}^{m} a_{ij}\wc{i} \text{ (cuando $1 \leq j \leq n$)}
    \end{align*}
    Definimos la matriz de T respecto a las bases $\base , \basep$ como:
    \begin{align*}
        \begin{bmatrix} T \end{bmatrix}_{\basep}^{\base}=(a_{ij}) \quad (\text{con }1\leq i\leq m,1\leq j\leq n)
    \end{align*}
    Si \incluye{V}{W} y $\base=\basep$ en particular denotaremos
    $\begin{bmatrix} T \end{bmatrix}_{\base} =\begin{bmatrix} T \end{bmatrix}_{\base}^{\basep}$

\end{definition}

\begin{theorem}
    \deftl\\
    Si $\base=\conjunto{\vc{1},\cdots,\vc{n}},\basep=\conjunto{\wc{1},\cdots,\wc{m}}$ bases ordenadas de V y W
    respectivamente
    \begin{align*}
        \ida \cb \cdot \cbs{v}{\base}{} = \cbs{\T{v}}{\basep}{} \quad \forall v \in V 
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{align*}
        \T{\vc{j}}=\sum_{i=1}^{m} a_{ij}\wc{i} \text{ , } \cb=(a_{ij})
    \end{align*}
    Sea $v \in V \ida \exists!$ escalares $x_1,\cdots,x_n$ tales que $v=x_1\vc{1}+\cdots+x_n\vc{n}$
    $\ida \cbs{v}{\base}{}=(x_1,\cdots,x_n)$
    \begin{align*}
            &\ida \T{v}=x_1\T{v_1}+\cdots+x_n\T{v_n}= \sum_{j=1}^{n}x_j\T{\vc{j}}\\
            &=\sum_{j=1}^{n}\sum_{i=1}^{m}x_j (a_{ij}w_i)=\sum_{j=1}^{n}(\sum_{i=1}^{m}x_j a_{ij})w_i\\
            &\ida \cbs{\T{v}}{\basep}{}=
            \underline{
            \begin{pmatrix}
            \sum_{i=1}^{n}a_{1j}x_j \:,\: \sum_{i=1}^{n}a_{2j}x_j \:,\: \cdots \:,\: \sum_{i=1}^{n}a_{mj}x_j    
            \end{pmatrix}
            }
            \quad (\star)
    \end{align*}
    Por otra parte
    \begin{align*}
        \cb \cdot \cbs{v}{\basep}{}=
        \begin{pmatrix}
            a_{11}x_1 + \cdots + a_{1n}x_n\\
            \ddots\\
            a_{m1}x_1 + \cdots + a_{mn}x_n
        \end{pmatrix}=
        \begin{pmatrix}
            \sum_{i=1}^{n}a_{1j}x_j\\
            \vdots\\
            \sum_{i=1}^{n}a_{mj}x_j
        \end{pmatrix}
        \quad (\star)
    \end{align*}
    Por lo tanto $\cbs{\T{v}}{\basep}{}=\cb\cbs{v}{\basep}{}$ tenemos lo queriamos ver.
\end{proof}

\begin{proposition}
    Sea V \espvec de dimension finita. Sean $\base,\basep$ bases de V. Entonces:
    \begin{align*}
        \cbs{v}{\basep}{}=\cbs{Id}{\basep}{\base} \cdot \cbs{v}{\base}{} \quad \forall v \in V
    \end{align*}
\end{proposition}

\begin{definition}
    Sea V \espvec de dimension finita. Sean $\base,\basep$ dos bases ordenadas de V.
    Denotaremos $P=\cbs{Id}{\basep}{\base}$ como la matriz de cambio de base de $\base$ a $\basep$. 
\end{definition}

\begin{theorem}
    Sean V,W,Z espacios vectoriales sobre $\K$. Sean $T:V\to W, S:W\to Z$ \tl y sean
    $\base_V,\base_W,\base_Z$ bases de V,W,Z respectivamente. Entonces:
    \begin{align*}
        &\cbs{S \circ T}{\base_Z}{\base_V}=\cbs{S}{\base_Z}{\base_W} \cdot \cbs{T}{\base_W}{\base_V} 
    \end{align*}
\end{theorem}
\begin{proof}
    Sea $v \in V$ arbitrario:
    \begin{align*}
        &\cbs{T}{\base_W}{\base_V} \cdot \cbs{v}{\base_V}{} = \cbs{\T{v}}{\base_W}{}
        \ida \cbs{S}{\base_Z}{\base_W} \cdot \cbs{\T{v}}{\base_W}{} = \cbs{S(\T{v})}{\base_Z}{}\\\\
        &\cbs{S \circ T}{\base_Z}{\base_V} \cdot  \cbs{v}{\base_V}{} = \cbs{S \circ \T{v}}{\base_Z}{}\\\\
        &\ida \cbs{S(\T{v})}{\base_Z}{} = \cbs{S \circ \T{v}}{\base_Z}{} \text{ (por definicion de composicion)}
    \end{align*}
\end{proof}
\newpage
\begin{proposition}
    Sea V \espvec , $\base = \conjunto{v_1,\cdots,v_n}$ y sea $T:V\to V$ \tl\\
    Entonces:
    \begin{enumerate}
        \item $\cbs{S\circ T}{\base}{}=\cbs{S}{\base}{} \cdot \cbs{T}{\base}{}$
        \item Si $I:V\to V$ es la identidad $\ida \cbs{Id}{\base}{}=Id$
        \item Si T es inversible $\ida \cbs{T}{\base}{}$ es inversible y
        $\cbs{T^{-1}}{\base}{}=\cbs{T}{\base}{}^{-1}$
    \end{enumerate}
\end{proposition}
\begin{proposition}
    Sea V \espvec de dimension finita, sean $\base,\basep$ dos bases de V.
    \begin{align*}
        &\text{Entonces si } P=\cbs{Id}{\basep}{\base} \ida P^{-1}=\cbs{Id}{\base}{\basep}
    \end{align*}
\end{proposition}

\begin{theorem}
    \deftl\\
    Sean $\base_V,\basep_V$ dos bases de V y sean  $\base_W,\basep_W$ dos bases de W.\\
    Sean $P=\cbs{Id}{\base_V}{\basep_V} (\basep_V\to\base_V)$
    y $Q=\cbs{Id}{\base_W}{\basep_W} (\basep_W\to\base_W)$. Entonces:
    \begin{align*}
        \cbs{T}{\basep_W}{\basep_V}= Q^{-1} \cdot \cbs{T}{\base_W}{\base_V} \cdot P
    \end{align*}
\end{theorem}
\begin{proof}
    Sea $v \in V$ expresado en base $\basep_V$
    \begin{align*}
        &\cbs{T}{\basep_W}{\basep_V} \cdot \cbs{v}{\basep_V}{} = \cbs{\T{v}}{\basep_W}{}\\\\
        &(Q^{-1} \cdot \cbs{T}{\base_W}{\base_V} \cdot P )\cdot \cbs{v}{\basep_V}{}
        = (Q^{-1} \cdot \cbs{T}{\base_W}{\base_V}) \cdot \cbs{v}{\base_V}{}\\
        &= Q^{-1} \cdot  \cbs{\T{v}}{\base_W}{} = \cbs{\T{v}}{\basep_W}{}
    \end{align*}
    Por lo tanto tenemos lo queriamos ver $\cbs{T}{\basep_W}{\basep_V}= Q^{-1} \cdot \cbs{T}{\base_W}{\base_V} \cdot P$
\end{proof}

\begin{theorem}
    Sea \espvec , $T:V \to V$ \tl , sean $\base,\basep$ base de V. Sea P la matriz de cambio de base de $\basep$ a $\base$
    $(P=\cbs{Id}{\base}{\basep})$. Entonces: \underline{$\cbs{T}{\basep}{}= P^{-1} \cdot \cbs{T}{\base}{} \cdot P$}
\end{theorem}

\newpage
\begin{definition}
    Dado una matriz $A \in \mtx{n}{n}$ dados $1\leq i,j \leq n$ definimos la matriz $A(i \tq j)$ como la matriz $(n-1)\times (n-1)$
    que se obtiene eliminando la fila $i$ y la columna $j$ de A.
\end{definition}
\begin{definition}
    Sea $n \in \mathbb{N}$ , $A \in \mtx{n}{n}$ el determinante de A se define de la siguiente forma:
    \begin{enumerate}
        \item Si $n=1$ , $A=\begin{pmatrix} a \end{pmatrix} \ida det(A)=a$
        \item Si $n>1$ (desarrollamos por la primera columna de A)
        \begin{align*}
            det(A)= \sum_{i=1}^{n}\: (-1)^{i+1} \cdot a_{i1} \cdot det(A(i \tq 1))
        \end{align*}
    \end{enumerate}
\end{definition}
\begin{theorem}
    Si $A \in \mtx{n}{n}$ es una matriz triangular $\ida det(A)= a_{11} \times a_{22} \cdots \times a_{nn}$ (el producto de la diagonal).
\end{theorem}
\begin{proof}
    \begin{align*}
        &A=\begin{pmatrix}
            \begin{array}{ccccc}
            a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
            0      & a_{22} & a_{23} & \cdots & a_{2n} \\
            0      & 0      & a_{33} & \cdots & a_{3n} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & 0      & \cdots & a_{nn}
            \end{array}
        \end{pmatrix}
        \ida det(A)=a_{11}\cdot det(1\tq 1)+ \cdots + 0\cdot det(n\tq 1)\\
        &A(1\tq 1)=\begin{pmatrix}
            \begin{array}{ccc}
            a_{22} & \cdots & a_{2n} \\
            \vdots & \ddots & \vdots \\
            0      & \cdots & a_{nn}
            \end{array}
        \end{pmatrix}
        \ida \text{ nuevamente la matriz es triangular superior}
    \end{align*}
    Por lo tanto $det(A)= a_{11}\times a_{22}\times \cdots \times a_{nn}$.
\end{proof}
\begin{proposition}
    Sea $A\in \mtx{n}{n}$. Entonces:
    \begin{itemize}
        \item Si $A=Id \ida det(A)=1$
        \item Si A es una MERF $\ida det(A)=\begin{cases} 1 \text{ si A no tiene filas nulas}\\0 \text{ c.c}\end{cases}$
    \end{itemize}
\end{proposition}

\begin{theorem}
    Sea $A \in \mtx{n}{n}$ , $B=$ matriz que se obtiene de A por medio de una operacion elemental por filas $(e_j(A)=B)$. Entonces:
    \begin{enumerate}
        \item Si $e= e^{c}_{i} \ida det(B)= c\cdot det(A)$
        \item Si $e= e^{c}_{i,j} \ida det(B)= det(A)$
        \item Si $e= e_{i,j} \ida det(B)= -det(A)$
    \end{enumerate}
\end{theorem}
\begin{proof}
    Veamos la prueba para $A \in \mtx{2}{2}:$
        \begin{enumerate}
            \item $B=\left\{\begin{smallmatrix} a_{11} & a_{12}\\ ca_{21} & ca_{22} \end{smallmatrix} \right\}
            \ida det(B)= a_{11}+c(a_{22}) - c(a_{21})a_{12} = c(a_{11}a_{22}-a_{21}a_{12}) = c \cdot det(A)$
            \item $B=\left\{ \begin{smallmatrix} a_{11} + ca_{21} & a_{12} + ca_{22}\\ a_{21} & a_{22} \end{smallmatrix} \right\}
            \ida det(B)= (a_{11} + ca_{21}) a_{22} - (a_{12} + ca_{22}) a_{21}\\
            \ida det(B)= (a_{11}a_{22}-a_{21}a_{12}) + (c(a_{21}+a_{22})-c(a_{21}+a_{22}))= det(A)$
            \item $B=\left\{\begin{smallmatrix} a_{21} & a_{22}\\ a_{11} & a_{12} \end{smallmatrix} \right\}
            \ida det(B)= a_{21}a_{12}-a_{11}a_{22} = -(a_{11}a_{22}-a_{21}a_{12}) = -det(A)$
        \end{enumerate}
\end{proof}

\begin{theorem}
    Sean $A,B \in \mtx{n}{n} \ida det(A \cdot B)=det(A) \cdot det(B)$    
\end{theorem}
\begin{proposition}
    Si $A \in \mtx{n}{n}$ es inversible $\ida det(A)\neq0$ y $det(A^{-1})=\frac{1}{det(A)}$
\end{proposition}
\begin{proposition}
    Sea $A \in \mtx{n}{n}$. Sean $E_1,\cdots,E_r$ matrices elementales y sea $B=E_r \cdots E_1A$.
    Entonces $det(B)=det(E_1)\times \cdots \times det(E_r)\times det(A)$ y se cumple lo siguiente:
    \begin{enumerate}
        \item Si B tiene alguna fila nula $\ida det(A)=0$
        \item Si B es una MERF sin filas nulas $\ida det(A)=\frac{1}{det(E_1)\times\cdots\times E_r}$
    \end{enumerate}
\end{proposition}
\begin{theorem}
    Sea $A \in \mtx{n}{n}$. Entonces A es inversible $\iff det(A)\neq 0$
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\ida):$ Como A es inversible $\exists A^{-1}$ talque $A\cdot A^{-1}=Id$ y como vimos anteriormente
        \begin{align*}
            &det(Id)=1=det(A)\times \underset{\neq 0}{\underbrace{det(A^{-1})}}\\
            &\ida det(A)\neq 0
        \end{align*}
        \item $(\vuelta):$ Sea M la MERF $\sim A$ $\ida \exists$ matriz inversible E (producto de elementales) talque:
        \begin{align*}
            &A=E\cdot M \ida A \text{ inversible} \iff M \text{ inversible}\\
            &\iff M=Id \text{ (no tiene filas nulas)}\\
            &\iff det(M)=1 \neq 0 \iff det(A)\neq 0
        \end{align*}
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $A \in \mtx{n}{n}$ inversible $\ida A^{-1}=\frac{1}{det(A)} \cdot adj(A)$ donde $adj(A)$ es la matriz:\\
    $(adj(A))_{ij}= (-1)^{i+j} \cdot det(A(j \tq i))$
\end{theorem}

\newpage

\begin{definition}
    \defaut.
    \begin{enumerate}
        \item $\lambda \in \K$ se dice autovalor de T si $\exists v \in V , v\neq0$ tal que $\T{v}=\lambda\cdot v$.
        \item Si $\lambda$ autovalor, cada vector $v\in V$ que satisface $\T{v}=\lambda\cdot v$ se llama
        autovector de T.
        \item Sea $W_\lambda = \conjunto{v\in V : \T{v}=\lambda\cdot v}$ el autoespacio de autovalor $\lambda$
        (o el espacio propio asociado a $\lambda$). 
    \end{enumerate}
\end{definition}
\begin{theorem}
    \defaut. Entonces son equivalentes:
    \begin{enumerate}
        \item $\lambda \in \K$ es autovalor de T
        \item $det(T-\lambda\cdot I)=0$
    \end{enumerate}
\end{theorem}
\begin{proof}
    Si $\lambda$ es autovalor $\ida Nu(T-\lambda I)\neq \conjunto{0}$ por lo tanto $T-\lambda I$ no es inversible
    $\ida det(T-\lambda I)=0$.
\end{proof}

\begin{definition}
    Sean $A,B \in \mtx{n}{n}$ se dicen semejantes si $\exists C \in \mtx{n}{n}$ inversible:  $A=C \cdot B \cdot C^{-1}$\\
    Lo denotaremos $A \sim B$.
\end{definition}

\begin{definition}
Sea \( A \in \mathbb{K}^{n \times n} \). Entonces, los autovalores, autovectores y autoespacios de $A$
coinciden con los de la transformación lineal $T_A : \K^n \to \K^n$  dada por $T_A(x) = Ax$.
\end{definition}

\begin{definition}
    \defaut.
    \begin{itemize}
        \item T es diagonalizable si $\exists \base$ base de V tal que $\cbs{T}{\base}{}$ es diagonal.
        \item $A \in \mtx{n}{n}$ es diagonalizable si A es semejante a D $(\exists C \text{ inversible}: C^{-1}AC=D)$.
    \end{itemize}
\end{definition}

\begin{theorem}
    $T:V \to V$ es diagonalizable $\iff \exists \base$ base de V formada por autovectores de T:
    \begin{align*}
        \exists \base=\conjunto{\vc{1},\cdots,\vc{n}} : \T{\vc{j}}=\lambda_j\vc{j} \text{ , para algun } \lambda_j \in \K
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\vuelta):$ $\base=\conjunto{\vc{1},\cdots,\vc{n}}$ base , $\T{\vc{j}}=\lambda_j\vc{j}$
        \begin{align*}
            \cbs{T}{\base}{}=\begin{pmatrix}
                \lambda_1 & 0         & \cdots & 0     \\
                0         & \lambda_2 & \cdots & 0      \\
                \vdots    & \vdots    & \ddots & \vdots  \\
                0         & 0         & \cdots & \lambda_n\\
            \end{pmatrix}
            \text{diagonal} \ida T \text{ es diagonalizable}
        \end{align*}
        \item $(\ida):$ Si T es diagonalizable $\ida \exists \base=\conjunto{\vc{1},\cdots,\vc{n}}$ base de V :
        $\cbs{T}{\base}{}$ es diagonalizable
        \begin{align*}
            &\ida \T{\vc{1}}=\lambda_1\vc{1} + 0\cdot \vc{2}+\cdots + 0\cdot \vc{n}= \lambda_1\vc{1}\\
            &\text{En general: } \T{\vc{j}}=\lambda_j\vc{j} \text{ , donde cada \vc{j} es un autovector}
        \end{align*}
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $T:V\to V$ \tl , sean $\vc{1},\cdots,\vc{r}$ autovectores de T (no nulos) asociados a autovalores distintos
    $\ida \conjunto{\vc{1},\cdots,\vc{r}}$ es \li
\end{theorem}
\begin{proof}
    Veamos por induccion, caso \conjunto{\vc{1}} es claramente \li , supongamos que vale el caso
    \conjunto{\vc{1},\cdots,\vc{r-1}}.
    Basta ver que \conjunto{\vc{1},\cdots,\vc{r}} es \li por lo tanto veamos:
    \begin{align}
        \underline{c_1\vc{1} + \cdots c_{r}\vc{r}=0} \ida c_j=0 \: \forall j
    \end{align}
    Sabemos que todos los $\vc{j}\neq0$, ahora aplicando T tenemos:
    \begin{align}
        \T{c_1\vc{1}+\cdots c_{r}\vc{r}}=\underline{c_1\cdot\lambda_1\vc{1} + \cdots+ c_{r}\cdot\lambda_{r}\vc{r}=0}
    \end{align}    
    Si multiplicamos $(1) \times \lambda_{r}$ y se lo restamos a $(2)$ obtenemos:
    \begin{align}
        c_1\cdot(\lambda_1 - \lambda_{r} )\vc{1} + \cdots
        +c_{r-1}\cdot(\lambda_{r-1} - \lambda_{r} )\vc{r-1} + c_{r}\cdot (\lambda_{r}-\lambda_{r})\vc{r}=0
    \end{align}
    Como sabemos cada \vc{j} esta asociados a un autovalor distintito (3) y por hipotesis sabemos que
    \conjunto{\vc{1},\cdots,\vc{r-1}} es \li
    \begin{align}
        \ida (\lambda_i - \lambda_{r})\neq0 , \forall i\leq r-1 \ida c_i=0 \:,\:\forall i\leq r-1
    \end{align}
    Si juntamos (1) y (4) tenemos que:
    \begin{align*}
        \underset{0}{\underbrace{c_1}}\vc{1} + \underset{0}{\underbrace{c_2}}\vc{2} +
        \cdots + \underset{0}{\underbrace{c_{r-1}}}\vc{r-1} + c_{r}\vc{r}=0
    \end{align*}
    Por definicion \vc{r} es no nulo $\ida c_r =0$. Por lo tanto \conjunto{v_1,\cdots,v_r}  es \li
\end{proof}
\end{document}
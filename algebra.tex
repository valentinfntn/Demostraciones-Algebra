\pdfminorversion=4
\documentclass[]{article}

% Packages/Macros %
\usepackage{amssymb,latexsym,amsmath}     % Standard packages
\usepackage{hyperref,comment,enumitem}

% Margins %
\addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{1.00in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\topmargin}{-.50in}

\newtheorem{theorem}{Teorema}
\newtheorem{proposition}{Proposicion}[theorem]
\newtheorem{definition}{Definicion}
\newenvironment{proof}{\noindent{\bf Prueba:}}{$\hfill \Box$ \vspace{10pt}}  

\newcommand{\iSection}[1]{
  \phantomsection
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\K}{
    \mathbb{K}
}
\newcommand{\ida}{\Longrightarrow}
\newcommand{\vuelta}{\longleftarrow}
\newcommand{\base}{\mathcal{B}}


\begin{document}

\newcommand{\W}[1]{\text{$W_{#1}$}}
\newcommand{\mtx}[2]{\text{$\mathbb{K}^{#1 \times #2}$}}
\newcommand{\espvec}{\text{ $\mathbb{K}-$espacio vectorial }}
\newcommand{\espgen}[1]{\text{$\langle #1 \rangle$}}
\newcommand{\incluye}[2]{\text{$#1 \subseteq #2$}}
\newcommand{\conjunto}[1]{\text{$\{ {#1} \}$}}
\newcommand{\li}{L.I.\:}
\newcommand{\tl}{T.L.\:}
\newcommand{\tq}{\text{ $|$ }}
\newcommand{\T}[1]{\text{$T({#1})$}}
\newcommand{\novacio}{\text{$\neq\emptyset$}\:}
\newcommand{\cero}{\text{$\vec{0}$}}
\newcommand{\deftl}{\text{Sean $V,W$ $\mathbb{K}-$espacio vectoriales, sea $T: V\rightarrow W$ transformacion lineal.}\:}
\newcommand{\N}{\text{$Nu(T)$}\:}
\newcommand{\I}{\text{$Im(T)$}\:}
\newcommand{\dm}[1]{\text{$dim(#1)$}\:}
\newcommand{\vc}[1]{\text{$v_{#1}$}}
\newcommand{\wc}[1]{\text{$w_{#1}$}}

\title{Demostraciones Algebra}
\maketitle

\begin{theorem}
    Sean $A,B \in \mtx{m}{n}$  matrices equivalentes por filas, entonces el sistema
    de ecuaciones $Ax=0$ y $Bx=0$ tienen exactamente las mismas soluciones. 
\end{theorem}
\begin{proof}
    Si $A \sim B \Longrightarrow \exists$ una sucesion de matrices tal que
    $A=A_0 \rightarrow A_1 \rightarrow \dots \rightarrow A_n=B$, donde cada $A_j$ se obtiene por
    medio de una operacion elemental por filas.\\
    Por lo tanto basta probar que $A_jx=0$ y $A_{j+1}x=0$.
    \begin{itemize}
        \item \underline{Caso $e_r^c$}: $a_{r1}x_1+a_{r2}x_2+ \dots +a_{rn}x_n = 0 \iff
        c \cdot a_{r1}x_1+c \cdot a_{r2}x_2+ \dots +c \cdot a_{rn}x_n = 0$, pero como $c \neq 0$
        $\Longrightarrow c \cdot (a_{r1}x_1+a_{r2}x_2+ \dots +a_{rn}x_n) = 0$, por lo tanto 
        ambos sistemas son iguales.
        \item \underline{Caso $e_{r,s}$}: es trivial pues ambas filas $r,s$ ya eran iguales a 0
        y lo siguen siendo.
        \item \underline{Caso $e_{r,s}^c$}: $(r + c \cdot s) = (a_{r1}+c\cdot a_{s1})x_1+
        (a_{r2}+c\cdot a_{s2})x_2+ \dots +(a_{rn}+c\cdot a_{sn})x_n = 0$ de la misma formas que en
        el primer caso como las filas $r,s$ son iguales a 0 por lo tanto la nueva fila r tambien lo es.
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $A \in \mtx{m}{n}$ con $m<n \Longrightarrow$ el sistema $Ax=0$
    tiene soluciones no triviales.
\end{theorem}
\begin{proof}
    Sea $R$ la MERF equivalente a $A \Longrightarrow$ los sistemas $Ax=0$ y $Rx=0$ tienen
    exactamente las mismas soluciones. Sea $r=$ la cantidad de filas no nulas de $R$
    $\Longrightarrow r \leq m$ y por lo tanto $r < n$ $\Longrightarrow$ hay $n-r>0$
    variables libres, por lo tanto hay soluciones no triviales.
\end{proof}

\begin{theorem}
    Sea $A \in \mathbb{K}^{n \times n}$. Entonces $A$ es equivalente por filas a las $Id \iff$
    $Ax=0$ tiene unicamente la solucion trivial.
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow):$ Si $A \sim Id$, estas tienen exactamente las mismas soluciones.
        Por lo tanto como $Idx=0$ admite unicamente la solucion trivial queda probado.
        \item $(\Longleftarrow):$ Sea $R$ la MERF $\sim A \Longrightarrow$ el sistema $Rx=0$
        tiene unicamente la solucion trivial. Sea $r=$ la cantidad de filas no nulas de $R \Longrightarrow$
        $n-r=0$ porque no tienen variables libres. Entonces cada fila $i$ tiene un 1 en la columna $k_i$
        por lo tanto $R=Id$.
    \end{itemize}
\end{proof}

\begin{theorem}
    Propiedades de la multiplicacion de matrices:
    \begin{enumerate}
        \item $A \in \mtx{m}{n}, B \in \mathbb{K}^{n\times p},C \in \mathbb{K}^{p\times q}$
        $\Longrightarrow (AB)C=A(BC)$.
        \item $A \in \mtx{m}{n} \Longrightarrow Id_mA = Id_nA = A$.
        \item $A,A^\prime \in \mtx{m}{n}, B,B^\prime \in \mathbb{K}^{n\times p} \Longrightarrow$
        $(A+A^\prime)B = AB+A^\prime B$ y $A(B+B^\prime) = AB+AB^\prime$.
        \item $A \in \mtx{m}{n}, B \in \mathbb{K}^{n\times p},\lambda \in \mathbb{K}$
        $\Longrightarrow \lambda \cdot (AB) = (\lambda A)B= A(\lambda B)$ 
    \end{enumerate}
\end{theorem}
\newpage
\begin{theorem}
    Sea $e$ una operacion elemental por filas y sea $E=e(Id)$ la matriz elemental asociada.
    Entonces para toda $A \in \mtx{n}{n}$ se cumple que $e(A)=E \cdot A$. 
\end{theorem}
\begin{proof}
    Tenemos que el elemento $i,j$ de $e(A)$ es el mismo que el de la matriz $EA$ para cada
    operacion elemental, osea $(e(A))_{ij}=(EA)_{ij}$.
    \begin{itemize}
        \item \underline{Caso $e_r^c$}:
        \begin{align*}
            &\text{Sabemos que } (e(A))_{ij}=
            \begin{cases}
                A_{ij} \text{ si } i \neq r\\
                cA_{ij} \text{ si } i = r
            \end{cases}\\
            &\text{Veamos }(EA)_{ij}=\sum_{k=1}^{m}E_{ik}A_{kj}
            \text{ (si $i \neq k \Longrightarrow E_{ik}=0$)}\\
            &= E_{ii}A_{ij}=
            \begin{cases}
                A_{ij} \text{ si } i \neq r\\
                cA_{ij} \text{ si } i = r
            \end{cases}\\
        \end{align*}
        \item \underline{Caso $e_{r,s}$}:
        \begin{align*}
            &\text{Sabemos que }(e(A))_{ij}=
            \begin{cases}
                A_{ij} \text{ si } i \neq r,s\\
                A_{sj} \text{ si } i = r\\
                A_{rj} \text{ si } i = s
            \end{cases}\\
            &(EA)_{ij}=\sum_{k=1}^{m} E_{ik}A_{kj}
            \text{ , donde } E_{ik}=
            \begin{cases}
                1 \text{ si } i=k \vee i=r,s \vee k=r,s\\
                0 \text{ caso contrario}
            \end{cases}\\
            &\text{Veamos $(EA)_{ij}$ en cada caso:}
            \begin{cases}
                \text{si } i \neq r,s \Longrightarrow (EA)_{ij} = A_{ij}\\
                \text{si } i=r \Longrightarrow (EA)_{ij} = E_{is}A_{sj} = A_{sj}\\
                \text{si } i=s \Longrightarrow (EA)_{ij} = E_{ir}A_{rj} = A_{rj}
            \end{cases}
        \end{align*}
        \item \underline{Caso $e_{r,s}^c$}:
        \begin{align*}
            &\text{Sabemos que } e(A)_{ij}=
            \begin{cases}[within]
                A_{ij} \text{ si } i \neq r\\
                A_{rj} + cA_{sj} \text{ si } i=r
            \end{cases}\\
            &(EA)_{ij}=\sum_{k=1}^{m} E_{ik}A_{kj}
            \text{ , donde } E_{ik}=
            \begin{cases}
                1 \text{ si } i=k\\
                c \text{ si } i=r \wedge j=s\\
                0 \text{ caso contrario}
            \end{cases}\\
        \end{align*}
    \end{itemize}
\end{proof}
def
\begin{theorem}
    Sean $A,B \in \mtx{n}{n}$:
    \begin{enumerate}
        \item Si $A$ es inversible $\Longrightarrow A^{-1}$ tambien lo es y $(A^{-1})^{-1}=A$.
        \item Si $A,B$ son inversibles $\Longrightarrow AB$ es inversible y $(AB)^{-1}=B^{-1}A^{-1}$. 
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item $A \cdot A^{-1} = A^{-1} \cdot A = Id \Longrightarrow A^{-1}$ inversible y $(A^{-1})^{-1}$ es $A$.
        \item $(AB)\cdot (B^{-1}A^{-1})=A(BB^{-1})A^{-1} = A(Id)A^{-1}=AA^{-1}=Id$.
    \end{enumerate}
\end{proof}

\newpage

\begin{theorem}
    Toda matriz elemental $E$ es inversible.
\end{theorem}
\begin{proof}
    Sea $e$ la operacion elemental por fila correspondiente a $E$ y sea $e^{\prime}$ la operacion elemental inversa
    (sabemos que existe por teorema). Por lo tanto sea $E^{\prime}=e^{\prime}(Id)$
    \begin{align*}
        &Id=e^{\prime}(e(Id))=e^{\prime}(E)=E^{\prime}E\\
        &Id=e(e^{\prime}(Id))=e(E^{\prime})=EE^{\prime}\\
        &\Longrightarrow E \text{ es inversible y su inversa es } E^{\prime}
    \end{align*}
\end{proof}
\begin{theorem}
    Sea $A \in\mtx{n}{n}$ entonces son equivalentes:
    \begin{enumerate}
        \item $A$ es inversible.
        \item $A$ es equivalente por filas a la $Id$.
        \item $A$ es producto de matrices elementales.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $[1 \Longrightarrow 2]$ Sea $R$ la MERF $\sim A \Longrightarrow$ existen matrices elementales
        $E_1, \dots ,E_k$ talque $R=E_k\cdots E_2E_1A$. Como las matrices elementales $E_j$ y $A$ son inversibles
        $\Longrightarrow R$ tambien lo es $\Longrightarrow R$ no tiene filas nulas por lo tanto $R=Id$.
        \item $[2 \Longrightarrow 3]$ Si $A \sim Id \Longrightarrow Id \sim A \Longrightarrow$ existen P productos
        de matrices elementales talque $A = P \cdot Id = E_1E_2 \cdots E_k \cdot Id$.
        \item $[3 \Longrightarrow 1]$ Supongamos $A=E_1 \cdots E_k$ donde $E_j$ es una matriz elemental. Como cada
        $E_j$ es inversible y el producto de matrices inversibles tambien lo es $\Longrightarrow$ A es inversible.
    \end{itemize}
\end{proof}

\begin{theorem}
    Sean $A,B \in\mtx{m}{n}$. Entonces $B$ es equivalente por filas a $A \iff \exists$ $P$ matriz inversible
    $m \times m$ talque $B=P \cdot A$  
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow):$ Si $B \sim A$ sabemos que $B=E_kE_{k-1}\cdots E_1$ y como cada $E_i$ es inversible
        el producto de matrices inversibles tambien lo es.
        \item $(\Longleftarrow):$ Sea $P$ inversible talque $B=PA$ como $P$ es producto de matrices elementales
        $\Longrightarrow B=E_k \cdots E_1 A \Longrightarrow$ $B$ se obtiene de $A$ haciendo operaciones elementales
        $\Longrightarrow B \sim A$.  
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $A\in\mtx{n}{n}$. Entonces son equivalentes: 
    \begin{enumerate}
        \item $A$ es inversible.
        \item El sistema $Ax=0$ tiene una unica solucion (la trivial).
        \item $\forall \: b \in\mathbb{K}^{n\times 1}$ el sistema no-homogeneo $Ax=b$ tiene una unica solucion.
    \end{enumerate}
\end{theorem}
\newpage
\begin{proof}
    \begin{itemize}
        \item $[1 \iff 2]$ Sabemos que $A$ es inversible $\iff A \sim Id \iff$ el sistema $Ax=0$ tiene
        como unica solucion la trivial.
        \item $[1 \Longrightarrow 3]$ Sea $b \in\mathbb{K}^{n\times 1}$ , como $A$ es inversible $\Longrightarrow$
        $\exists A^{-1}$. Por lo tanto sea $x_0= A^{-1}b \in\mathbb{K}^{n\times 1}$ $\Longrightarrow Ax_0=A(A^{-1}b)=b$.\\
        Veamos que es unica, para eso supongamos que existe otra solucion $x_1 \Longrightarrow Ax_1=b$
        $\Longrightarrow Ax_1=b=Ax_0$ ahora multiplicamos por la inversa $\Longrightarrow A^{-1}Ax_0=A^{-1}Ax_1$
        $\Longrightarrow x_0=x_1$
        \item $[3 \Longrightarrow 2]$ Como tiene solucion para todo $b$ tomo $b=0$ por lo tanto, obviamente, tiene una unica
        solucion por hipotesis. 
    \end{itemize}
\end{proof}

\newpage

\begin{theorem}
    Si $W \subseteq V$ y $W\novacio$. Entonces $W$ es un $\mathbb{K}-$subespacio vectorial de $V$ $\iff \forall v,w \in W$
    y $\forall c \in \mathbb{K}$ el vector $[c \cdot v + w] \in W$ 
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow):$ Si $W$ es un subespacio vectorial y $c \in \mathbb{K}$ , $v,w \in W \Longrightarrow$
        $c \cdot v \in W \Longrightarrow c \cdot v + w \in W$
        \item $(\Longleftarrow):$ Supongamos que $\forall v,w \in W$ y  $\forall c \in \mathbb{K} : c \cdot v + w \in W$ veamos contiene
        al $\cero$, que es cerrado para la suma y el producto por escalar.
        \begin{enumerate}
            
            \item Como $W \novacio \Longrightarrow \exists w \in W \Longrightarrow (-1) \cdot w + (1) \cdot w \in W$
            $\Longrightarrow \cero \in W$.
            \item Tomamos $c=1$ por lo tanto $(1) \cdot v + w = v + w \in W$ por lo tanto la suma esta bien definida.
            \item Como $\cero \in W$ tomamos $w=\cero$ por lo tanto $c \cdot v + \cero = c \cdot v \in W$
            entonces el producto esta bien definido.
        \end{enumerate}
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec. Entonces la interseccion de subespacios de $V$ es un subespacio vectorial de $V$.
\end{theorem}
\begin{proof}
    \begin{align*}
        \text{Sea $\{ W_i \}_{i \in I}$ , donde $W_i$ es un subespacio vectorial de V. Entonces sea } W = \bigcap_{i \in I}W_i.
    \end{align*}
    Para ver que $W$ es un subespacio veamos que si $v,w \in W$ , $c \in \K \ida c \cdot v + w \in W$.\\
    Si $v,w \in W \ida v,w \in W_i$ para todo $i \in I$ y como todo $W_i$ es un subespacio $\ida c \cdot v + w \in W_i$
    $\ida c \cdot v + w \in W$.
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec y sean $v_1, \dots , v_k \in V$.\\
    Entonces $W=\{ c_1v_1+ \dots + c_kv_k | c_i \in \K\}$ = Conjunto de todas la combinaciones lineales,
    es un subespacio vectorial de $V$.
\end{theorem}
\begin{proof}
    Tomemos $v,w \in W$ , $c \in \K$ veamos $c \cdot v + w \in W$.
    \begin{align*}
        &\text{Sean } v= c_1v_1+ \dots c_kv_k \text{ y } w = d_1v_1 + \dots d_kv_k \ida \\
        &c \cdot v + w = (c \cdot c_1v_1+ \dots c \cdot c_kv_k) + (d_1v_1 + \dots d_kv_k)\\
        &= (c \cdot c_1 + d_1)v_1 + \dots (c \cdot c_k + d_k)v_k \\
        &\ida c \cdot v + w \text{ es una combinacion lineal, por lo tanto} \in W
    \end{align*}
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec de dimension finita y $W$ subespacio de $V$. Todo subconjunto \li de $W$ es
    finito y se puede completar a una base. 
\end{theorem}
\begin{proof}
    Si $S_0 \subseteq W$ es \li $\ida S_0$ es un conjunto L.I de $V$ y si $dim(V)=n$, sabemos que $|S_0|\leq n$,
    osea es finito. Queremos extenderlo a una base de la siguiente forma:
    \begin{itemize}
        \item Si $S_0$ genera $W$ ya es una base.
        \item Si $S_0$ no genera $W \ida \exists w_1 \in W$ talque $w_1 \notin \espgen{S_0}$, vimos que si
        agregamos algo que no esta en el espacio generado este conjunto sigue siendo \li\\
        Repetimos este paso hasta algun $S_k = S_0 \cup w_1 \cup \cdots \cup w_k$ talque $\espgen{S_k} = W$.
        Como este conjunto es por definicion \li y genera este es una base.  
    \end{itemize}
\end{proof}

\begin{theorem}
    Sea $V$ un \espvec, sea $S=\{ v_1, \dots ,v_m \}$ un conjunto de generadores de V. Entonces existe
    $B \subseteq S$ que es una base de $V$. 
\end{theorem}
\begin{proof}
    Vamos a definir inductivamente subconjuntos $G_j \subseteq S$ ($j=1,\dots,m$) que sean \li
    \begin{itemize}
        \item $(j=1):$
        \begin{align*}
            G_1=
            \begin{cases}
                \{v_1\} \quad si \quad v_1 \neq 0\\
                \emptyset \quad si \quad v_1 = 0
            \end{cases}
            \ida G_1 \text{ es \li}
        \end{align*}
        \item $(j=2):$
        \begin{align*}
            G_2=
            \begin{cases}
                G_1 \cup {v_2} \quad si \quad v_2 \notin \espgen{G_1}\\
                G_1 \quad\quad si \quad v_2 \in \espgen{G_1}
            \end{cases}
            \ida G_2 \text{ es \li en cualquier caso}
        \end{align*}
        \item $(j+1):$
        \begin{align*}
            G_{j+1}=
            \begin{cases}
                G_j \cup {v_{j+1}} \quad si \quad v_{j+1} \notin \espgen{G_j}\\
                G_j \quad\quad si \quad v_{j+1} \in \espgen{G_j}
            \end{cases}
            \ida \text{ en ambos casos $G_{j+1}$ es \li}
        \end{align*}
    \end{itemize}
    $\ida G_m$ es \li y $\espgen{G_m} = \{ v_1,\dots,v_m \} = V \ida \base = G_m \subseteq S$ y la dimension
    de $V = |G_m|$.
\end{proof}

\begin{theorem}
    Sean $\W{1} , \W{2}$ dos subespacios de $V$:
    \begin{enumerate}
        \item $\W{1} + \W{2}$ es un subespacio de $V$.
        \item $\W{1} + \W{2}$ es el menor subespacio (respecto a la inclusion) que contiene a $\W{1}+\W{2}$.
        \item Si $\{ v_i \}_{i \in I}$ es un conjunto de generadores de \W{1} y $\{ w_j \}_{j \in J}$ es un conjunto
        de generadores de $\W{2} \ida \{ v_i \}_{i \in I} \cup \{ w_j \}_{j \in J}$ es un conjunto de generadores de $\W{1}+\W{2}$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item $\W{1} + \W{2} \novacio$ pues $\cero \in \W{1}$ y $\cero \in \W{2}\ida \cero=\cero+\cero \in \W{1}+\W{2}$.
        Sean $v_1,v_2 \in \W{1}+\W{2}$ queremos ver que $c \cdot v_1 + v_2 \in \W{1} + \W{2}$.
        \begin{align*}
            &v_1 = x_1 + y_1 \text{ , } v_2= x_2 + y_2 \text{ con } x_1,x_2 \in \W{1} \subseteq V
            \text{ y } y_1,y_2 \in \W{2} \subseteq V\\
            &\ida c \cdot v_1 + v_2 = c \cdot (x_1 + y_1) + (x_2 + y_2) = 
            (cx_1 + x_2) \in  \W{1} + (cy_1 + y_2) \in \W{2}\\
            &\ida c \cdot v_1 + v_2 \in \W{1} + \W{2}
        \end{align*}
        \item $\W{1} \subseteq \W{1} + \W{2}$ pues si $w_1 \in \W{1} \ida w_1 + \cero \in \W{1}+\W{2}$ y del mismo modo con \W{2}.
        Por lo tanto $\W{1} \cup \W{2} \subseteq \W{1} + \W{2}$, basta ver que es el menor subespacio que lo contiene.\\
        Sea $U$ un subespacio de $V$ talque $\W{1} \cup \W{2} \subseteq U$, queremos ver que $\W{1} + \W{2} \subseteq U$.\\
        Sea $v \in \W{1} + \W{2} \ida v=x+y$ con $x \in \W{1} \subseteq U$, $y \in \W{2} \subseteq U$ osea $v \in U$
        $\ida \W{1} + \W{2} \subseteq U$.
        \item $\{v_i\}_{i \in I}$ genera \W{1}, $\{w_j\}_{j \in J}$ genera $\W{2} \ida S=$
        $\{v_i\}_{i \in I} \cup \{w_j\}_{j \in J}$ genera $\W{1} + \W{2}$.
        \begin{align*}
            &\text{Sea } v \in \W{1} + \W{2} \ida v=x+y \text{ con } x \in \W{1} , y \in \W{2}\\
            &\ida x=\sum_{i \in I} \alpha_i v_i \: , \: y=\sum_{j \in J} \beta_j w_j
            \quad (\text{ con } \alpha_i,\beta_j \in \K)\\
            &\ida v=x+y=\sum_{i \in I}\alpha_i v_i + \sum_{j \in J}\beta_j w_j\\
            &\text{Por lo tanto $v$ es combinacion lineal de elementos de $S$}
        \end{align*}
    \end{enumerate}
\end{proof}
\begin{theorem}
    Sea $V$ un \espvec , sean $\W{1},\W{2}$ subespacios de $V$ y de dimension finita.\\
    Entonces $\W{1}+\W{2}$ es un subespacio y $dim(\W{1}+\W{2}) = dim(\W{1}) + dim(\W{2}) - dim(\W{1} \cap \W{2})$
\end{theorem}
\begin{proof}
    $\W{1}\cap\W{2}$ es un subespacio de \W{1} (y de \W{2}) por lo tanto es de dimension finita.
    Sea \conjunto{u_1,\dots,u_k} base de $\W{1} \cap \W{2} \ida$ \conjunto{u_1,\dots,u_k} es \li en \W{1}
    $\ida$ se puede extender a una base de \W{1}, analogamente con \W{2}.\\
    Osea existen \conjunto{v_1,\dots,v_n} $\subseteq$ \W{1} tales que \conjunto{u_1,\dots,u_k,v_1,\dots,v_n}
    es una base de \W{1}. De la misma forma existen \conjunto{w_1,\dots,w_m} $\subseteq$ \W{2} tales que
    \conjunto{u_1,\dots,u_k,w_1,\dots,w_m} es base.\\
    $\ida \W{1}+\W{2}$ es generado por $\base= \conjunto{u_1,\dots,u_k,v_1,\dots,v_n,w_1,\dots,w_m}$
    es $\base_1 \cup \base_2$. Veamos que este conjunto es \li
    \begin{align*}
        &\text{Si } \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{n} b_i v_i + \sum_{i=1}^{m} c_i w_i = 0\\
        &\ida \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{n} b_i v_i = \underline{- \sum_{i=1}^{m} c_i w_i} \in \W{1}\cap\W{2}\\
        &\text{Como \conjunto{u_1,\dots,u_k} es base de } \W{1}\cap\W{2} \ida - \sum_{i=1}^{m} c_i w_i = \sum_{i=1}^{k} d_iu_i\\
        &\text{Reemplazando } \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{n} b_i v_i + \sum_{i=1}^{k} (-d_i)u_i = 0\\
        &\sum_{i=1}^{k} (a_i-d_i)u_i + \sum_{i=1}^{n} b_i v_i = 0
    \end{align*}
    Como \conjunto{u_1,\dots,u_k,v_1,\dots,v_n} es base de \W{1} el conjunto es $\li\ida a_i=d_i$ y $b_i=0 ,\:\forall\: i$.\\
    Volviendo a la sumatoria con este resultado sabemos que:
    \begin{align*}
        \sum_{i=1}^{k} a_i u_i + \sum_{i=1}^{m} c_i w_i = 0
    \end{align*}
    Como \conjunto{u_1,\dots,u_k,w_1,\dots,w_m} es base de \W{2} el conjunto es \li $\ida$ todos los escalares son 0.
    Osea $\beta = \conjunto{u_1,\dots,u_k,v_1,\dots,v_n,w_1,\dots,w_m}$ es \li y por lo tanto es base de \W{1}+\W{2}
    \begin{align*}
        &\ida dim(\W{1}+\W{2})<\infty \text{ y } dim(\W{1}+\W{2})=k+n+m\\
        &dim(\W{1})=k+n \text{ , } dim(\W{2})=k+m \text{ , } dim(\W{1}\cap\W{2})=k\\
        &\ida dim(\W{1}+\W{2}) = dim(\W{1})+dim(\W{2})-dim(\W{1}\cap\W{2})
    \end{align*}
\end{proof}

\begin{proposition}
    Si $V=\W{1} \oplus \W{2} \ida$ para cada $v\in V$ : $\exists$ unicos elementos $x \in \W{1} , y \in\W{2}$
    tales que $v=x+y$.
\end{proposition}
\begin{proof}
    Como $V=\W{1}+\W{2}$ sabemos que existen $w_1 \in \W{1}, w_2 \in \W{2}$ tales que $v=w_1=w_2$.
    Si ademas $v=u_1+u_2 \ida \cero= v-v= (w_1+w_2)-(u_1+u_2)=(w_1-u_1)+(w_2-u_2)$
    $\ida w_1-u_1 = w_2-u_2 \in \W{1}\cap\W{2}= \conjunto{0}$ (pues es suma directa) $\ida w_1=u_1 , w_2=u_2$.
\end{proof}

\begin{theorem}
    Sean \W{1},\W{2} subespacios vectoriales de $V$ y sean $\base_1,\base_2$ bases de \W{1} y \W{2}.\\
    Entonces son equivalentes:
    \begin{enumerate}
        \item $V=\W{1}\oplus\W{2}$.
        \item $\base = \base_1 \cup \base_2$ es una base de $V$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $[1 \ida 2]:$ Asumimos que $V=\W{1}\oplus\W{2} \ida \base = \base_1 \cup \base_2$ es un conjunto de generadores de
        $\W{1}+\W{2}$ bastar ver que es \li\\
        Sabemos que $dim(V)=dim(\W{1}\oplus\W{2})=dim(\W{1})+dim(\W{2})-dim(\conjunto{0})$
        \begin{align*}
            &\sum_{i \in I} a_i v_i + \sum_{j \in J} b_j w_j = 0 \ida
            \underset{\in \W{1}}{\sum_{i \in I} a_i v_i} = \underset{\in \W{2}}{-\sum_{j \in J} b_j w_j} \ida \W{1}\cap\W{2}=\conjunto{0}\\
            &\sum_{i \in I} a_i v_i =  \cero \text{ , } \sum_{j \in J} b_j w_j = \cero\\
            &\ida a_i=0,b_j=0 \text{ $\forall i \in I, \forall j \in J$ pues ambos $v_i,w_j$ son base y por lo tanto son } \li
        \end{align*}
        \item $[2 \ida 1]:$ Asumimos que $\base=\base_1 \cup \base_2$  base de $V \ida V=\W{1}+\W{2}$ pues sabemos que:
        \begin{align*}
            v=\sum_{i \in I}a_i v_i + \sum_{j \in J}b_j w_j
        \end{align*}
        Necesitamos ver que $\W{1}\cap\W{2}=\conjunto{0}$ para ver que la suma es directa. Sea $v\in\W{1}\cap\W{2}:$
        \begin{align*}
            &\ida v=\sum_{i \in I}a_i v_i = \sum_{j \in J}b_j w_j\\
            &\ida 0=\sum_{i \in I}a_i v_i - \sum_{j \in J}b_j w_j\\
            &(\text{como ambos conjuntos son L.I. esto solo pasa si } a_i=0 \:\forall i \in I,b_j=0 \:\forall j \in J)\\
            &\ida v=0\ida \W{1}\cap\W{2}=\conjunto{0}
        \end{align*}
    \end{itemize}
\end{proof}

\newpage
\begin{definition}
    Dados V,W espacio vectoriales sobre $\K$.\\
    Una transformacion lineal de V en W es una funcion, $T:V \rightarrow W$ que satisface:
    \begin{itemize}
        \item $T(v+w)=T(v)+T(w)$ \quad $\forall v,w \in V$
        \item $T(c\cdot v)= c\cdot T(v)$ \quad $\forall v\in V, \forall c \in \K$
    \end{itemize}
    De esas propiedades se deducen las siguientes:
    \begin{itemize}
        \item $T(\cero_V)=\cero_W$
        \item $T(-v)=-T(v)$ \quad $\forall v \in V$
    \end{itemize}
\end{definition}

\begin{theorem}
    Sean V,W \espvec , $T:V\rightarrow W$ una \tl
    \begin{enumerate}
        \item Si $\incluye{U}{V}$ es un subespacio de $V \ida T(U)$ es un subespacio de $W$.
        \item Si $\incluye{Z}{W}$ es un subespacio de $W \ida T^{-1}(Z)=\conjunto{v \in V \tq T(w)\in Z}$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Sea $\T{U} = \conjunto{w \in W \tq \exists v \in U \text{ con } \T{v}=w} \novacio$ , veamos
        que \T{U} es subespacio:
        \begin{align*}
            &\text{Como } \cero \in U \text{ (por ser subespacio) y } \T{\cero}=0 \ida \cero \in \T{U}\\
            &\text{Sean } w_1,w_2 \in \T{U}: \ida \exists v_1,v_2 \in U \text{ tales que } w_1=\T{v_1},w_2=\T{v_2}\\
            &\ida c\cdot w_1 + w_2 = c\T{v_1}+\T{V_2} = \T{c\cdot v_1 + v_2}\\
            &\text{Como } c\cdot v_1+v_2 \in U \ida c\cdot w_1 + w_2 \in \T{U}
        \end{align*}
        \item $\cero \in T^{-1}(Z)$ pues $\T{0} = \cero \in Z$ por lo tanto $T^{-1}(Z) \novacio$.
        Sean $v_1,v_2 \in T^{-1}(Z) , c \in \K$ veamos que $c\cdot v_1 + v_2 \in T^{-1}(Z)$:
        \begin{align*}
            &\T{c\cdot v_1+v_2}=c\T{v_1}+\T{v_2} \in defZ\\
            &\ida c\cdot v_1+v_2 \in T^{-1}(Z)
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}
    \deftl\\
    Entonces:
    \begin{enumerate}
        \item \N es un subespacio de V.
        \item \I es un subespacio de W.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item
        \begin{align*}
            &\N\novacio \text{ , pues } \cero\in\N \text{ porque } \T{\cero}=\cero\\
            &\text{Sean } v_1,v_2 \in \N , c \in\K\\
            &\ida \T{cv_1+v_2}=c\T{v_1}+\T{v_2}=\cero \ida c\cdot v_1 + v_2 \in \N
        \end{align*}
        \item
        \begin{align*}
            &\incluye{\I}{W} , \I\novacio \text{ pues } \cero\in\I \text{ , } \cero\in\T{\cero}\\
            &\text{Sean } w_1,w_2 \in \I , c\in\K \\
            &\ida \exists v_1,v_2 \in V \text{ tales que } \T{cv_1+v_2}=c\T{v_1}+\T{v_2}=c\cdot w_1 + w_2 \in \I 
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}
    \deftl\\
    (con $dim(V)<\infty$) Entonces:
    \begin{align*}
        dim(V)=dim(\I)+dim(\N)
    \end{align*}
\end{theorem}
\begin{proof}
    Sea \conjunto{v_1,\dots,v_k} base de \incluye{\N}{V}. Como el conjunto es \li podemos completarlo
    a una base de $V$. Sean \incluye{\conjunto{v_{k+1},\dots,v_n}}{V} tal que \conjunto{v_1,\dots,v_k,v_{k+1},\dots,v_n}
    es una base de $V$.\\
    Probemos \conjunto{\T{\vc{k+1}},\dots,\T{\vc{n}}} es base de \I :
    \begin{align*}
        &\text{Sea } w \in \I \ida \exists v \in V \text{ tal que } w=\T{v} \ida v=\sum_{j=1}^{n}c_j\vc{j}\\
        &\T{v}=\sum_{j=1}^{n}c_j\T{\vc{j}}=\sum_{j=k+1}^{n}c_j\T{\vc{j}} \quad \text{ (porque $\T{\vc{j}}=0$ si $j\leq k$)}\\
        &\ida w \text{ es combinacion lineal de } \conjunto{\T{\vc{k+1}},\dots,\T{\vc{n}}}  
    \end{align*}
    Falta ver que el conjunto es $\li :$
    \begin{align*}
        &\ida b_{k+1}\T{\vc{k+1}}+\cdots+b_n\T{\vc{n}}=\underset{\vc{0}}{\T{\underbrace{b_{k+1}v_{k+1}+\cdots+b_nv_n}}}=0\\
        &\ida \vc{0}\in \N \ida \vc{0}=\sum_{j=k+1}^{n}b_jv_j=\sum_{j=1}^{k}c_iv_i
        \text{ pues \conjunto{\vc{1},\dots,\vc{k}} es base de \N}\\
        &\ida \sum_{j=k+1}^{n}b_jv_j+\sum_{j=1}^{k}(-c_i)v_i = 0
    \end{align*}
    Como la segunda parte es $\li \ida b_j=0$ , $\forall j=\conjunto{k+1,\cdots,n}$. Con esto probamos que
    \conjunto{\T{\vc{k+1}},\dots,\T{\vc{n}}} es \li y como genera es base de \I.\\
    Por lo tanto $\dm{\I}=n-k$ , ademas sabiamos que $\dm{\N}=k$
    \begin{align*}
        \ida \dm{\N}+\dm{\I}=k+(n-k)=n=\dm{V}
    \end{align*}
\end{proof}
\end{document}